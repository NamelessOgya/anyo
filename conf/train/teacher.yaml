batch_size: 32
max_epochs: 4
learning_rate: 0.0002
weight_decay: 0.01
accelerator: gpu # cpu or gpu
precision: bf16-mixed

devices: 1 # GPUの数
num_workers: 2 # DataLoaderのワーカー数 (Colab向けに削減)
val_check_interval: 0.5 # 0.5は半エポックごとに検証
log_every_n_steps: 50
accumulate_grad_batches: 4 # 32 * 4 = 128 (Matches iLoRA effective batch size)
