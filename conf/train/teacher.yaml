batch_size: 16
max_epochs: 4
learning_rate: 0.0008 # Reference: 8e-4
weight_decay: 0.01
accelerator: gpu # cpu or gpu
precision: bf16-mixed

devices: 1 # GPUの数
num_workers: 2 # DataLoaderのワーカー数 (Colab向けに削減)
val_check_interval: 0.5 # 0.5は半エポックごとに検証
log_every_n_steps: 50
accumulate_grad_batches: 8 # 16 * 8 = 128 (Matches iLoRA effective batch size)
