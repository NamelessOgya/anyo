model_type: ilora
llm_model_name: Qwen/Qwen1.5-1.8B-Chat # 小さなモデルでテスト
num_lora_experts: 4 # Reference: 4
lora_r: 8 # Reference: 8
lora_alpha: 32 # Reference: 32
lora_dropout: 0.1 # Reference: 0.1
hidden_size: 64 # ゲーティングネットワーク用
dropout_rate: 0.1
max_gen_length: 20
rec_model_checkpoint_path: /content/drive/MyDrive/rec/anyo/result/student_baseline_20251124_114208/checkpoints/student-baseline-epoch=00-val_recall@10=0.0028.ckpt # 事前学習済みSASRecモデルのチェックポイントパス
# teacher_checkpoint_path: null # 学習済みモデルのパス (run_distill, run_eval_allで使用)
use_flash_attention: True

use_torch_compile: False
use_gradient_checkpointing: True
use_qlora: False
skip_output_generation: false
subset_indices_path: null # Active Learning用のインデックスファイルパス

distill_lambda: 0.1 # Reverse Distillation Loss (Regularization) weight
use_item_embeddings_head: True # True: Use Student Embeddings as Head, False: Use Random Linear Head
distill_loss_type: "mse" # Options: mse, cosine, l1, huber, contrastive
distill_decay_type: "linear" # Options: none, linear, cosine, exponential
distill_min_lambda: 0.0 # Minimum value for distill_lambda after decay
distill_decay_steps: null # Number of steps to reach min_lambda. If null, use max_steps.
batch_size: 32 # Reference: 32 (Matches train.teacher.yaml default)
