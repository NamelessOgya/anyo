# Standard BIGRec Configuration
model_type: bigrec
llm_model_name: "meta-llama/Llama-2-7b-hf"
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
max_source_length: 512
max_target_length: 64
num_beams: 1
temperature: 0.0
top_p: 0.9
top_k: 40
warmup_steps: 20
item_embeddings_path: "data/ml-100k/item_embeddings.pt"
popularity_path: "data/ml-100k/popularity_counts.pt"
compute_item_embeddings: true
val_check_interval: 0.1
load_in_8bit: false
batch_size: 4
