# BIGRec Configuration
model_type: bigrec
llm_model_name: "meta-llama/Llama-2-7b-hf" # Changed from Qwen as requested
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
learning_rate: 3e-4
max_source_length: 512
max_target_length: 64
use_cot: false
max_history_items: 20 # Truncate history to last 20 items to speed up training

# Dataset
limit_data_rows: null # Set to integer for debugging
batch_size: 8
