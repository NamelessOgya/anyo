# 既存実装と参照リポジトリのロジックレベルの差分 (Logic-Level Difference from As-Is)

このドキュメントは、本プロジェクトの `src` ディレクトリ内の実装が、参照リポジトリ（iLoRA, DLLM2Rec）で提案されている**既存手法のロジックを再現できているか**に焦点を当て、その差分をまとめるための依頼文書です。アーキテクチャや実装の詳細な構造が異なっていても構いません。重要なのは、**アルゴリズムの主要なステップや数式が、本プロジェクトのコードでどのように実装され、既存ロジックの再現がなされているか**という点です。

---

## 1. 比較方針

*   **目的**: 本プロジェクトのコードが、`00_overview.md` に記載されている iLoRA および DLLM2Rec の「ロジック再現の方針」をどの程度満たしているかを客観的に評価する。
*   **粒度**: 各アルゴリズムの核となるロジック、数式、データフローの主要な変換ステップに焦点を当てる。コードの行単位の比較ではなく、アルゴリズムの再現性に着目する。
*   **対象**:
    *   **iLoRA ロジック**: `ref_repositories/iLoRA/` の主要なロジックと `src/teacher/` の対応する実装。
    *   **DLLM2Rec ロジック**: `ref_repositories/dllm2rec/` の主要なロジックと `src/distill/` の対応する実装。

---

## 2. iLoRA ロジックの再現性評価 (`src/teacher/` vs `ref_repositories/iLoRA/`)

### 2.1. iLoRA の主要ロジック (00_overview.md より)

*   **LLM + 複数 LoRA エキスパート + シーケンスごとのゲーティング。**
*   **`h_seq`（シーケンス表現） → softmaxゲート → LoRAパラメータを線形結合 → LLMに適用。**
*   **学習：次アイテム予測（CE loss）、学習対象は LoRA + ゲートのみ。**
*   **教師出力として DLLM2Rec 互換のランキング・スコア・埋め込みを出す。**

### 2.2. 評価項目と確認事項

以下の点について、本プロジェクトの `src/teacher/ilora_model.py` および関連ファイルが、参照実装のロジックを再現しているかを確認してください。

*   **ゲーティングネットワークの機能**:
    *   シーケンス表現 (`h_seq` に相当するもの) がどのように生成され、ゲーティングネットワークに入力されているか。
    *   ゲーティングネットワークが、複数の LoRA エキスパートに対する softmax 重みを正しく出力しているか。
*   **LoRA パラメータの線形結合**:
    *   ゲーティングネットワークの重みに基づいて、複数の LoRA エキスパートの出力（またはパラメータ自体）がどのように線形結合され、LLM に適用されているか。
    *   特に、`peft` ライブラリの利用方法が、参照実装の意図する動的な LoRA 結合ロジックを再現しているか。
*   **LLM への適用と次アイテム予測**:
    *   結合された LoRA が適用された LLM が、どのように次アイテムの予測（ロジット）を生成しているか。
    *   プロンプトエンジニアリングが、LLM の入力として適切に機能しているか。
*   **学習対象**:
    *   学習対象が LoRA パラメータとゲーティングネットワークのパラメータのみに限定されているか（LLM の基盤モデルはフリーズされているか）。
    *   損失関数が次アイテム予測の Cross-Entropy Loss を使用しているか。
*   **教師出力**:
    *   `get_teacher_outputs` メソッドが、DLLM2Rec 互換のランキングスコアと埋め込みを正しく出力しているか。
    *   特に、埋め込みが LLM の最終隠れ状態を適切に集約したものであるか。

---

## 3. DLLM2Rec ロジックの再現性評価 (`src/distill/` vs `ref_repositories/dllm2rec/`)

### 3.1. DLLM2Rec の主要ロジック (00_overview.md より)

*   **教師ランキング＋スコアを用いたランキング蒸留 loss。**
*   **教師埋め込み vs 生徒埋め込みの距離を縮める埋め込み蒸留 loss。**
*   **CE loss（通常学習）＋ KD loss の合成。**

### 3.2. 評価項目と確認事項

以下の点について、本プロジェクトの `src/distill/kd_losses.py`, `src/distill/trainer_distill.py` および関連ファイルが、参照実装のロジックを再現しているかを確認してください。

*   **ランキング蒸留損失**:
    *   `RankingDistillationLoss` が、教師モデルと生徒モデルのランキングスコア（ロジット）を用いて、論文で提案されているランキング蒸留の数式を正しく実装しているか。
    *   温度パラメータ `T` の適用が適切か。
*   **埋め込み蒸留損失**:
    *   `EmbeddingDistillationLoss` が、教師モデルと生徒モデルの埋め込みを用いて、論文で提案されている埋め込み蒸留の数式（MSE または Cosine Similarity）を正しく実装しているか。
*   **損失の合成**:
    *   `DistillationTrainer` の `training_step` において、ランキング蒸留損失、埋め込み蒸留損失、および通常の Cross-Entropy Loss が、それぞれの重み (`ranking_loss_weight`, `embedding_loss_weight`, `ce_loss_weight`) に基づいて正しく合成されているか。
*   **蒸留サンプル選択ポリシー**:
    *   `selection_policy.py` で定義されているポリシー（例: `GroundTruthErrorPolicy`）が、蒸留に用いるサンプルを論文の意図に沿って選択しているか。

---

## 4. 評価結果の記載方法

上記の評価項目に基づき、各ロジックの再現性について以下の形式で記載してください。

*   **項目**: 評価対象のロジックの名称
*   **参照実装のロジック概要**: 参照リポジトリで提案されているロジックの簡単な説明（数式や主要ステップ）
*   **本プロジェクトの実装**: `src` ディレクトリ内の対応するコードが、そのロジックをどのように実装しているかの説明
*   **差分と再現性評価**:
    *   ロジックレベルでの差分がある場合は具体的に記述。
    *   既存ロジックが再現できているか、できていない場合はその理由と影響を考察。
    *   再現できている場合は、その根拠を簡潔に述べる。

---

## 5. 参照リポジトリの確認

`ref_repositories/dllm2rec/` の中身はまだ詳細に確認されていません。評価を行うエージェントは、まずこのディレクトリの主要なコード（特に損失関数や学習ループに関連する部分）を確認し、DLLM2Rec のロジックを把握してください。
