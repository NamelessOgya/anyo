# 7. Agent-Gamma 作業計画

## 7.1. 課題

教師モデルの学習時に発生する `RuntimeError: CUDA driver error: operation not supported` を解決し、知識蒸留パイプラインの正確な実行を可能にする。

## 7.2. 作業計画 (PyTorch Lightning不使用版)

(このセクションは、手動ループ実装の記録として残すが、現在は `7.5` のPyTorch Lightningベースのアプローチに回帰している)

*   ... (内容は変更なし) ...

## 7.3. 作業計画 (コンテナ再ビルドによる環境問題の切り分け)

(このセクションは、環境問題解決の記録として残す)

*   ... (内容は変更なし) ...

## 7.4. 作業計画 (OOMエラーの調査と解決)

(このセクションは、手動ループ実装時のOOMエラー調査の記録として残す)

*   ... (内容は変更なし) ...

## 7.5. 作業計画 (PyTorch Lightningへの回帰と再検証)

**背景:**
手動での学習ループ実装 (`tmp/implement_without_pl` ブランチ) において、損失が `NaN` になる問題が発生し、デバッグが困難であった。また、ユーザーの要望により、より安定していたPyTorch Lightningを使用したバージョンにコードベースを復元した。ただし、`Dockerfile` と `docs` は最新の状態を維持している。
このセクションでは、復元されたコードベースで、以前の `CUDA driver error` が解消されているか、また `NaN` の問題が再発しないかを確認する。

### 7.5.1. コードベースの復元

*   [x] `git checkout` を使用し、`src` ディレクトリなどをPyTorch Lightningを使用していたコミット (`5f0caf344cbf7c5584125105e56bd136852b2182`) の状態に復元した。
*   [x] `tmp/implement_without_pl` ブランチから、最新の `Dockerfile` と `docs` ディレクトリの内容を現在のブランチにマージした。

### 7.5.2. 動作確認 (PyTorch Lightning版)

*   [x] `run_teacher.py` (PyTorch Lightning版) を実行し、`CUDA driver error` が解消されているか確認した。
    *   **結果:** `CUDA driver error` は解消された。しかし、新たに2つの問題が発生した。
        1.  `test_loss` が `NaN` になる。
        2.  教師出力の生成時に `RuntimeError: Expected all tensors to be on the same device` が発生する。

### 7.5.3. `NaN` / `inf` 問題のデバッグ

*   [x] **データ数の削減:** 問題の切り分けを容易にするため、`conf/dataset/movielens.yaml` の `limit_data_rows` を `100` に設定して検証を行った。
*   [x] **デバイス不一致エラーの修正:** `run_teacher.py` の教師出力生成部分で、モデルとバッチデータを明示的に同じデバイスに送るように修正し、`RuntimeError` を解消した。
*   [x] **学習率の調整:** `conf/train/teacher.yaml` の `learning_rate` を `1e-3` から `1e-4` に下げて実行したが、`NaN` 問題は解決しなかった。
*   [x] **精度の強制:** `pl.Trainer` に `precision=32` を設定し、`float16` に起因する数値不安定性の可能性を排除したが、`NaN` 問題は解決しなかった。
*   [x] **勾配クリッピングの導入:** `pl.Trainer` に `gradient_clip_val=1.0` を設定し、勾配爆発を抑制しようと試みたが、`NaN` 問題は解決しなかった。
*   [x] **デバッグプリントによる内部状態の確認:**
    *   `test_step` および `validation_step` にデバッグプリントを追加し、`logits` が `NaN` または `inf` になっていることを確認した。
    *   これにより、問題は学習の早い段階（最初のエポックの検証フェーズ）で発生しており、モデルの出力が発散していることが示唆された。

### 7.5.4. 現在の仮説と次のステップ

*   **仮説:**
    これまでの調査から、問題の根本原因は、LLMからの出力である `last_hidden_state` が訓練中に発散（非常に大きな値または `inf`/`NaN` になる）し、それが後続の `item_prediction_head` を経て `logits` の `inf`/`NaN` を引き起こしている可能性が高い。学習率の調整や勾配クリッピングだけでは解決できない、より根本的な不安定性がモデルの初期状態または構造に存在すると考えられる。

*   **次のステップ:**
    1.  `validation_step` 内で `last_hidden_state` の統計情報（最小値、最大値、平均値）をプリントし、実際に値が発散しているかを確認する。
    2.  発散が確認された場合、原因をさらに切り分ける。
        *   **初期化の問題:** `item_prediction_head` の初期化方法を変更してみる。
        *   **事前学習済みモデルの問題:** ロードしている `rec_model` が不安定性の原因である可能性を調査する。
        *   **アーキテクチャの問題:** モデルの構造自体に不安定性を引き起こす要因がないかレビューする。

### 7.5.5. 最終的な動作確認

*   [ ] `NaN` 問題を解消し、`run_teacher.py` がエラーなく最後まで実行できることを確認する。
*   [ ] `limit_data_rows` と `learning_rate` を元の値に戻し、完全なデータセットで学習が安定して行えることを確認する。
*   [ ] 学習ログ、保存されたモデル、評価結果が期待通りであることを確認する。

## 7.6. 開発サイクル改善: プログレスバーの導入

**背景:**
現状、`pl.Trainer` の `enable_progress_bar` は `False` に設定されている。これは、デフォルトのプログレスバーがコンソール出力を乱し、視認性が悪かったためである。しかし、プログレスバーがないと学習の進捗が分かりにくく、開発効率が低下している。`rich` ライブラリなどを用いて、よりクリーンで視認性の高いプログレスバーを導入することで、この問題を解決する。

### 7.6.1. プログレスバーのベストプラクティス調査

*   [x] PyTorch Lightning と `rich` を組み合わせた際のプログレスバー実装のベストプラクティスを調査する。
*   [x] 調査結果を、出典を明記した上で `docs/research/01_progressbar_best_practice.md` にまとめる。

### 7.6.2. 実装

*   [x] `pyproject.toml` に `rich` を依存関係として追加する。
*   [x] `pytorch_lightning.callbacks.RichProgressBar` を使用して、カスタムのプログレスバーコールバックを実装する。
*   [x] `src/exp/run_distill.py` に、新しいプログレスバーコールバックを適用する。
*   [x] `src/exp/run_teacher.py`, `src/exp/run_student_baseline.py` の2つの学習スクリプトに、新しいプログレスバーコールバックを適用する。
*   [x] `pl.Trainer` の `enable_progress_bar=False` の設定を削除、または `True` に変更する。

### 7.6.3. テスト

*   [ ] データ数とエポック数を少なく設定した上で、3つの学習スクリプトをそれぞれ実行する。
*   [ ] 新しいプログレスバーがコンソールを乱すことなく、進捗を明確に表示することを確認する。
