# Embedding Imitation Methods (Embedding Distillation)

TeacherモデルがStudentモデルのItem Embeddingを模倣（Imitation）するための手法について調査しました。
単純なMSE以外にも、ベクトルの方向性や分布、関係性を考慮した様々な指標が存在します。

## 1. Mean Squared Error (MSE)
- **概要**: 対応するEmbeddingベクトル間のユークリッド距離の二乗を最小化します。
- **特徴**: ベクトルの「大きさ」と「方向」の両方を厳密に一致させようとします。最も一般的で実装が容易です。
- **数式**: $L_{MSE} = \frac{1}{N} \sum_{i=1}^{N} ||E_T^{(i)} - E_S^{(i)}||_2^2$

## 2. Cosine Embedding Loss
- **概要**: ベクトル間のコサイン類似度を最大化（コサイン距離を最小化）します。
- **特徴**: ベクトルの「方向」の一致を重視し、「大きさ（ノルム）」の違いを許容します。推薦システムや意味検索では、ベクトルの向きが重要であることが多いため有効です。
- **数式**: $L_{Cos} = 1 - \frac{1}{N} \sum_{i=1}^{N} \frac{E_T^{(i)} \cdot E_S^{(i)}}{||E_T^{(i)}|| ||E_S^{(i)}||}$

## 3. L1 Loss (Mean Absolute Error)
- **概要**: ベクトル間の差の絶対値を最小化します。
- **特徴**: MSEに比べて外れ値（大きく乖離したEmbedding）の影響を受けにくい（ロバスト）です。スパースな解を導きやすい特性もあります。
- **数式**: $L_{L1} = \frac{1}{N} \sum_{i=1}^{N} |E_T^{(i)} - E_S^{(i)}|$

## 4. Huber Loss
- **概要**: 誤差が小さいときはMSE、大きいときはL1のように振る舞う損失関数です。
- **特徴**: MSEの微分可能性とL1のロバスト性を兼ね備えています。学習の安定性が高い場合があります。
- **数式**: 
  $L_{\delta}(a) = \begin{cases} \frac{1}{2}a^2 & \text{for } |a| \le \delta, \\ \delta (|a| - \frac{1}{2}\delta), & \text{otherwise.} \end{cases}$

## 5. Contrastive Loss (InfoNCE)
- **概要**: 対応するアイテム（正例）の類似度を上げ、それ以外のアイテム（負例）との類似度を下げるように学習します。
- **特徴**: 個々のベクトルの絶対的な値よりも、Embedding空間内での「相対的な位置関係（構造）」を保存することに優れています。
- **実装イメージ**: Teacherのアイテム $i$ とStudentのアイテム $i$ を正例ペアとし、Studentの他のアイテム $j (\neq i)$ を負例とします。

## 推奨実装方針
これらの手法を `distill_loss_type` パラメータで切り替えられるように実装します。
デフォルトは現状の `mse` とし、実験的に他のLossを試せるようにします。
