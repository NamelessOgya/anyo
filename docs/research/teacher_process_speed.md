# 教師モデル学習速度の低下に関する調査レポート

## 1. 調査の背景と目的

**問題:**
本プロジェクトで実装された教師モデル（iLoRAベース）の学習において、1エポックの完了に約10時間という非常に長い時間を要している。これは、参照元である `iLoRA` リポジトリでの実行速度と比較して著しく遅く、現実的な実験サイクルを妨げている。

**目的:**
使用GPU（A100）やデータ生成方法が共通であるにもかかわらず、なぜこのような性能差が生まれるのか。その原因を特定するため、プロジェクトの実装と `iLoRA` 参照リポジトリの間の、処理レベルでの差異を調査する。

## 2. 結論

調査の結果、学習速度の著しい低下は、単一の要因ではなく、主に以下の2つの要因の組み合わせによって引き起こされていると結論付けられる。

1.  **実効バッチサイズの大幅な違い（最有力要因）:**
    -   **プロジェクト:** 実効バッチサイズ `512`
    -   **iLoRA参照実装:** 実効バッチサイズ `128` (batch_size: 8, grad_accumulation: 16)
    -   プロジェクトのバッチサイズが参照実装の **4倍** であり、1ステップあたりの計算負荷が単純に4倍になっている。

2.  **QLoRAによる逆量子化のオーバーヘッド:**
    -   **プロジェクト:** `use_qlora: True` を設定し、4bit量子化を利用。
    -   **iLoRA参照実装:** `torch.bfloat16` (16bit) を利用。
    -   A100のような高性能GPUでは、ネイティブな16bit演算が非常に高速であるため、4bitから16bitへの逆量子化に伴うオーバーヘッドが、純粋な16bitでの学習よりも低速になるケースがある。これが追加のボトルネックとなっている可能性が高い。

**補足: 非効率なループ処理について**
調査の過程で、両方の実装（プロジェクトおよびiLoRA参照実装）に、バッチ内の各サンプルをPythonの`for`ループで処理するという、極めて非効率なコードパス (`wrap_emb` / `forward` 内での埋め込み置換処理) が存在することを発見した。この非効率な処理が、プロジェクト側の大きなバッチサイズによってさらに増幅され、深刻なパフォーマンス低下を招いている。

## 3. 詳細な調査結果

### 3.1. フェーズ1: 設定と実行時パラメータの比較

| パラメータ | プロジェクト (`ilora.yaml` + `teacher.yaml`) | `iLoRA` 参照実装 | 考察 |
| :--- | :--- | :--- | :--- |
| **LLM Model** | `meta-llama/Llama-2-7b-hf` | `.../llama2_7b_hf` | **同一**。速度差の原因ではない。 |
| **Quantization** | `use_qlora: True` (4-bit) | `bfloat16` (16-bit) | **速度低下の要因**。A100上での逆量子化オーバーヘッドが疑われる。 |
| **Effective BS** | **512** | **128** (8 * 16) | **速度低下の最大要因**。1ステップあたりの処理量が4倍。 |
| `num_lora_experts` | 3 | 4 (`num_moe`) | プロジェクトの方がわずかに速いはずであり、問題ではない。 |
| `num_workers` | 11 | 8 | プロジェクトの設定は妥当であり、データロードが主因とは考えにくい。 |

### 3.2. フェーズ2: モデルアーキテクチャとフォワードパスの分析

-   **ボトルネック処理の共通性:**
    プロジェクトの `src/teacher/ilora_model.py` の `forward` メソッドと、`iLoRA` 参照実装の `model/model_interface.py` の `wrap_emb` メソッドを比較した。
    その結果、両者とも**バッチサイズ分のPython `for` ループ**を回して、プロンプト内のプレースホルダートークンを対応するアイテム埋め込みに一つずつ置換していることが判明した。
    ```python
    # 両実装に共通する非効率なループ処理の例
    for i in range(batch_size):
        # ...
        # idx_tensor を使って input_embeds の特定の位置を置換
        modified_input_embeds[i, idx] = item_emb
    ```
    この設計自体が非効率だが、両実装で共通しているため、これが直接の性能"差"の原因ではない。しかし、プロジェクト側の大きなバッチサイズ(`512`)が、このループ処理の実行時間を参照実装(`128`)の4倍に引き延ばし、結果として全体のパフォーマンスを著しく悪化させている。

-   **LoRA実装の差異:**
    両者とも `peft` ライブラリをベースにしているが、プロジェクト側の実装は `_replace_module` などのカスタムラッパーが多く、複雑性が増している。これ自体がオーバーヘッドの一因である可能性は否定できないが、バッチサイズや量子化方式ほどのインパクトはないと推測される。

## 4. 今後のアクション

上記の調査結果から、学習速度を改善するために以下の段階的なアクションを提案する。

1.  **【最優先】バッチサイズの見直し:**
    -   `conf/train/teacher_production.yaml` （または関連する学習設定ファイル）の `batch_size` を `8` に、`pl.Trainer` の `accumulate_grad_batches` を `16` に設定し、**実効バッチサイズを `128` に統一する**。
    -   これにより、`iLoRA` 参照実装と条件を揃え、どの程度の速度改善が見られるかを確認する。

2.  **【推奨】QLoRA利用の再検討:**
    -   アクション1でまだ速度が不十分な場合、`conf/teacher/ilora.yaml` の `use_qlora` を `False` に設定し、`precision: "bf16-mixed"` を用いて `bfloat16` での学習を試す。
    -   これにより、逆量子化のオーバーヘッドが原因であった場合、さらなる速度向上が期待できる。（ただし、メモリ使用量が増加するため注意が必要）

3.  **【長期的改善】フォワードパスのベクトル化:**
    -   根本的な解決策として、`forward` メソッド内のPythonループを排除し、PyTorchの高度なインデックス操作や `torch.scatter` などを用いたベクトル化された処理にリファクタリングすることを検討する。これは両実装に共通するボトルネックであり、改善すれば両者のパフォーマンスを大幅に向上させることができる。
