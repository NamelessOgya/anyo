# 軽量LLMの調査と性能比較

## 1. 目的

現在、教師モデルで使用されている `meta-llama/Llama-2-7b-hf` (7B パラメータ) は、実験のイテレーションを回す上で実行時間が長すぎるという課題がある。
本ドキュメントは、この課題を解決するため、7Bパラメータ未満で、かつ十分な性能を持つ代替の軽量オープンソースLLMを調査し、その候補をリストアップすることを目的とする。

## 2. 調査概要

Web検索を通じて、Hugging Face Open LLM Leaderboardや各種技術ブログ、ベンチマーク記事を調査した。
特に、パラメータ数が7B未満でありながら、汎用的なテキスト生成や推論タスクで高い評価を得ているモデルに焦点を当てた。

## 3. 代替LLM候補

以下に、調査の結果見つかった有望なモデルをリストアップする。

---

### 候補1: Microsoft Phi-3-mini (3.8B)

-   **Hugging Face ID:** `microsoft/Phi-3-mini-4k-instruct`
-   **パラメータ数:** 38億
-   **特徴:**
    -   今回の調査で最も有望な候補。サイズと性能のバランスに優れる。
    -   多くのベンチマークで、自身より大きなモデルを上回る性能を示すと報告されている。
    -   特に論理的思考、コーディング、言語理解の能力で高く評価されている。
    -   最初のテスト対象として強く推奨される。

---

### 候補2: Google Gemma (2B)

-   **Hugging Face ID:** `google/gemma-2b-it`
-   **パラメータ数:** 20億
-   **特徴:**
    -   Googleの高性能モデル `Gemini` と同じ技術を基に開発された軽量モデル。
    -   Phi-3-miniよりもさらに小さく、高速な推論が期待できる。
    -   効率性と性能の両立を目指して設計されており、もしPhi-3-miniでも速度が不十分な場合の有力な選択肢となる。

---

### 候補3: DeepSeek Coder (1.3B)

-   **Hugging Face ID:** `deepseek-ai/deepseek-coder-1.3b-instruct`
-   **パラメータ数:** 13億
-   **特徴:**
    -   主にコーディングタスクに特化して訓練されているが、汎用的な対話能力も持つ。
    -   パラメータ数が非常に小さいため、候補の中で最も高速な実行が期待できる。
    -   本プロジェクトのプロンプト形式は比較的シンプルであるため、コーディング特化モデルでも十分に機能する可能性がある。速度を最優先で検証したい場合に適している。

---

### 候補4: Alibaba Qwen1.5 (1.8B)

-   **Hugging Face ID:** `Qwen/Qwen1.5-1.8B-Chat`
-   **パラメータ数:** 18億
-   **特徴:**
    -   非常に軽量でありながら、特に会話タスクにおいて優れた性能を発揮する。
    -   DeepSeek Coderと同様、速度が非常に速いことが期待される。
    -   多言語対応も特徴の一つ。

## 4. 推奨される次のステップ

1.  **モデルの選定:** 上記リストから、最初にテストするモデルを1つ選定する。**`microsoft/Phi-3-mini-4k-instruct`** から始めるのが最もバランスが良いと考えられる。
2.  **設定変更:** `conf/teacher/ilora.yaml` の `llm_model_name` を、選定したモデルのHugging Face IDに変更する。
3.  **実験の実行:** 教師モデルの学習スクリプト (`cmd/colab/11_run_teacher.sh`) を実行し、パフォーマンス（実行時間、メモリ使用量）と精度（validation loss, recall）を記録する。
4.  **評価:** Llama-2-7Bの結果と比較し、代替モデルの有効性を評価する。
