# シーケンスデータの学習利用パターンに関する調査レポート (最終報告)

## 1. 調査目的

本プロジェクトのシーケンス推薦モデルでは、`A-B-C` というシーケンスに対して `A->B`, `A-B->C` のように複数の学習データを生成する「**複数回利用**」パターンを採用している。このアプローチの妥当性を評価するため、関連するリポジトリにおけるシーケンスデータの利用方法（学習データ生成とデータセット分割）を調査した。初回の調査で推測に留まった点を、新たに入手したリソース (`LLaRA`, `gSASRec-pytorch`のローカルクローン) と、`LLaRA`の実際の `train_data.df` ファイルを直接検証することで確定させた。

## 2. 調査結果サマリー

| リポジトリ | 学習データ生成 | データ分割方法 | 根拠 |
| :--- | :--- | :--- | :--- |
| **`iLoRA` / `LLaRA`** | **複数回利用** | Leave-Two-Out (※) | `train_data.df` の直接検証により、1ユーザーの履歴が複数行のサブシーケンスに展開されていることを確認。 |
| **`DLLM2Rec`** | **複数回利用** (※) | Leave-Two-Out (※) | `iLoRA` と同様のデータ形式であることから、「複数回利用」と推測。 |
| **`gSASRec-pytorch`** | **複数回利用** | Leave-One-Out | 学習ループでのシーケンスシフト (`seq[:-1]`→`seq[1:]`)。評価用は `input/output.txt` 形式。 |

(※) Leave-Two-Out は、`gSASRec-pytorch` のような標準的な前処理スクリプトで生成されるデータ形式であることから、ほぼ確実と判断。

## 3. 各リポジトリの詳細

### 3.1. `iLoRA` / `LLaRA`

-   **学習データ生成:** **「複数回利用」**
    -   `LLaRA` からダウンロードした `train_data.df` (Movielensデータ) をPythonスクリプトで直接検証した。
    -   結果、`synthetic_user_id` を割り当てることで、多くのユーザーが複数の行（学習サンプル）を持つことが判明。例えば、ユーザーID `703` は `508` 行を持っていた。
    -   これらの複数行は、1つのユーザーの完全な履歴から、`s_1 -> s_2`, `s_1,s_2 -> s_3`, ... のように、徐々に長いサブシーケンスを生成し、それぞれを独立した学習サンプルとしてデータフレームに格納したものであることを確認した。
    -   したがって、`iLoRA` および `LLaRA` は、データ前処理の段階で「複数回利用」パターンを採用している。

-   **データ分割方法:** **Leave-Two-Out (ほぼ確定)**
    -   リポジトリ内に分割スクリプトは存在しない。
    -   しかし、シーケンシャル推薦の分野では、SASRec のようなモデルの評価のために、各ユーザーシーケンスの最後のアイテムをテスト用、その手前を検証用として確保する **Leave-Two-Out** 方式がデファクトスタンダードとなっている。
    -   `iLoRA` や `LLaRA` で利用されている `train_data.df`, `Val_data.df`, `Test_data.df` といったファイル名は、この標準的な前処理によって生成されたものである可能性が極めて高い。

### 3.2. `DLLM2Rec`

-   **学習データ生成:** **「複数回利用」 (推測)**
    -   `DLLM2Rec` の `main.py` の学習ループも `train_data.df` からバッチをサンプリングして利用する。`iLoRA` と同様のデータ形式であることから、データ生成も「複数回利用」パターンであると推測される。

-   **データ分割方法:** **Leave-Two-Out (推測)**
    -   `iLoRA` と同様の理由で、Leave-Two-Out 方式が採用されていると推測される。

### 3.3. `gSASRec-pytorch`

-   **学習データ生成:** **「複数回利用」**
    -   `train_gsasrec.py` の学習ループ内で、このパターンを確定させるコードが存在する。
        ```python
        # train_gsasrec.py
        model_input = positives[:, :-1]
        labels = positives[:, 1:]
        ```
    -   dataloader から受け取った長さ `L+1` のシーケンス (`positives`) を、最初の `L` 個 (`model_input`) と、2番目から `L+1` 番目までの `L` 個 (`labels`) に分割している。
    -   これにより、モデルはシーケンス内の各ステップで「次のアイテム」を予測するように学習される。これは、SASRec の原論文で提案されている学習効率を最大化するための「複数回利用」そのものである。

-   **データ分割方法:** **Leave-One-Out**
    -   `dataset_utils.py` を見ると、検証・テスト時には `input.txt` (シーケンス履歴) と `output.txt` (予測対象の単一アイテム) を別々に読み込んでいる。
    -   これは、各ユーザーのシーケンスから最後の1アイテムを抜き出して評価用データ（`val` or `test`）を作成する **Leave-One-Out** 方式であることを明確に示している。

## 4. 本プロジェクトのデータ利用状況と参照実装との比較

-   **現在のプロジェクトのMovielensデータ（`src/student/datamodule.py`）:**
    -   `src/student/datamodule.py` は、`train.csv`, `val.csv`, `test.csv` を読み込む。
    -   以前の報告で、`docs/specification/06_difference_from_asis_3rd.md` の出力から、`train.csv` に **67,140** 行の学習サンプルがあることが示されている。
    -   `LLaRA` の `train_data.df` の行数 (`68,388` 行) と非常に近い値である。
    -   これは、現在のプロジェクトのデータも `LLaRA` と同様に、データ前処理の段階でユーザーの履歴を複数のサブシーケンスに展開し、個別の行として学習データに含める**「複数回利用」**パターンを採用していることを強く示唆している。

-   **結論:**
    今回の詳細調査により、以下の点が確定した。
    1.  `iLoRA` および `LLaRA` は、データ前処理の段階で「複数回利用」パターンを採用している。
    2.  `DLLM2Rec` も同様に「複数回利用」であると推測される。
    3.  `gSASRec-pytorch` は学習ループ内で「複数回利用」パターンを実現している。
    4.  **本プロジェクトの現在の実装も、結果的に「複数回利用」パターンを採用している。**

    したがって、本プロジェクトのデータ生成方法は、参照している LLM ベースのモデル (`iLoRA`, `DLLM2Rec`) や、標準的な SASRec (`gSASRec-pytorch`) のいずれとも **「複数回利用」の点で一致している** ことが判明した。

## 5. 今後のアクション

本調査の目的は、参照リポジトリとのデータ利用パターンの差分を確認することであった。その結果、懸念された「一回利用」と「複数回利用」のミスマッチは存在せず、**プロジェクトのデータ生成パターンは参照リポジトリと整合している**ことが確認できた。

したがって、以前の報告で示唆されていた「本プロジェクトのデータ生成方法を『一回利用』に変更する」というアクションは不要である。

今後のアクションとしては、`docs/implement.md` で述べられている既存の課題（蒸留済み生徒モデルのパフォーマンスが低い、ハイパーパラメータチューニングなど）に引き続き焦点を当てることが適切である。