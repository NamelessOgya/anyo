# LoRA FinetuningにおけるOOM (Out of Memory) 問題に関する調査

## 1. はじめに
現在発生している `CUDA out of memory` エラーの原因を特定するため、LoRA (Low-Rank Adaptation) finetuningにおけるGPUメモリ要求に関する調査を行った。
現在の状況は、`facebook/opt-125m` という比較的小さなモデル（1億2500万パラメータ）を、バッチサイズ2、勾配チェックポインティング有効というメモリ効率的な設定で訓練しているにもかかわらず、16GB以上のVRAMを消費しOOMエラーが発生するという異常な状態である。

## 2. 一般的なLoRA FinetuningにおけるGPUメモリ要求
調査の結果、`opt-125m`モデルのLoRA finetuningにおける一般的なGPUメモリ要求は、現在の状況とは大きく異なることが判明した。

- **モデルサイズ**:
  - `float16`（半精度）でのモデルロード: 約240MB
- **訓練時の推定ピークメモリ使用量** (`float16`, Adam optimizer):
  - モデルパラメータ: ~240MB
  - 勾配: ~240MB
  - Adamオプティマイザの内部状態（2つのモーメント）: ~480MB
  - **合計: 約1GB**
  - これに加えて、活性化（activations）のためのメモリが必要となるが、勾配チェックポインティングを有効にしているため、その量は大幅に削減されるはずである。

**結論**: `opt-125m`のLoRA訓練が16GB以上のVRAMを消費するのは、**極めて異常**であり、単純なハイパーパラメータのミスマッチではなく、**メモリリークやライブラリの特定の使用法に起因するバグ**の可能性が非常に高い。

## 3. LoRAにおけるメモリ使用量に影響を与える主要な変数
LoRAのメモリ使用量は、以下の主要な変数に敏感である。

1.  **`batch_size`（バッチサイズ）**:
    - メモリ使用量に最も直接的な影響を与える。バッチサイズが大きいほど、より多くのサンプルを同時に処理するため、活性化のためのメモリが増加する。
    - 現在のプロジェクトでは、バッチサイズを`2`まで下げているため、これがOOMの主要因とは考えにくい。

2.  **`max_sequence_length`（最大シーケンス長）**:
    - メモリ使用量はシーケンス長に比例して増加する傾向がある。これは、Attentionメカニズムの計算量がシーケンス長の二乗（O(n^2)）で増加するためである。
    - 現在のプロジェクトでは`128`に設定されている。これは極端に長い値ではないが、もしメモリリークが存在する場合、シーケンス長がその影響を増幅させる可能性がある。

3.  **`lora_r`（LoRAのランク）**:
    - LoRAアダプタの訓練可能なパラメータ数を決定する。`r`が大きいほど、訓練パラメータが増え、メモリ使用量も増加する。
    - しかし、その増加量は比較的小さく、`r`を8から16に増やしても、メモリ全体に与える影響は通常、数MBから数十MB程度である。現在の16GBという異常な使用量を説明するものではない。

4.  **`target_modules`（LoRAを適用する層）**:
    - LoRAを適用する層（例: `q_proj`, `v_proj`だけでなく、`k_proj`, `o_proj`, `fc1`, `fc2`など）を増やすと、訓練パラメータ数が増加し、メモリ使用量が増える。
    - 現在のプロジェクトでは`q_proj`と`v_proj`のみを対象としており、標準的な設定である。

## 4. 考えられるOOMの原因と今後の調査方針
上記の調査から、現在のOOM問題は、以下のいずれか、または複数の組み合わせによって引き起こされている可能性が高い。

1.  **`transformers`ライブラリのバグまたは特定の挙動**:
    - `resize_token_embeddings`が、内部的に非効率なメモリ再確保を行っている、あるいは古いEmbeddingレイヤーを解放していない可能性がある。
    - `gradient_checkpointing`と`LoRA`、そして現在のモデル（`OPT`）の組み合わせにおいて、特定の条件下でメモリが適切に解放されないバグが存在する可能性がある。

2.  **PyTorch Lightningとの相互作用**:
    - PyTorch Lightningの訓練ループ内で、意図しないテンソルへの参照が保持され続け、ガベージコレクションが妨げられている可能性がある。

3.  **iLoRAの独自実装部分**:
    - 本プロジェクトの独自実装である`iLoRA`（特に`MoeLoraConfig`や`Linear`クラス）において、フォワードパスの計算時に中間生成物が意図せず保持され、メモリリークを引き起こしている可能性がある。特に、カスタムの`forward`関数内で`gate_weights`を扱っている部分が調査対象となる。

**今後のアクション**:
- **アクション1**: `resize_token_embeddings`の呼び出しを一時的にコメントアウトし、特殊トークンを追加しない状態でOOMが再現するか確認する。
- **アクション2**: PyTorch Lightningを使わず、素のPyTorchで最小限の訓練ループを実装し、同様のOOMが発生するかを切り分ける。
- **アクション3**: `iLoRA`の独自実装部分、特に`MoeLoraConfig`と`Linear`クラスの`forward`パスを詳細にレビューし、メモリプロファイラ（例: `torch.cuda.memory_profiler`）を使ってテンソルのライフサイクルを追跡する。
